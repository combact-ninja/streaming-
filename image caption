import numpy as np
from glob import glob
import cv2
from tqdm import tqdm
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM, Bidirectional, Conv2D, MaxPooling2D, \
        Flatten, add
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical, plot_model

def preprocess():
    path = 'DATA/Images/'
    all = glob(path + '/**')
    img_array = []
    for i in range(len(all)):
        img = cv2.imread(all[i])
        img = cv2.resize(img, (250,250))
        image = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))
        img_array.append(image)

    im = np.vstack((img_array))

    np.save('features.npy', im)


with open('DATA/captions.txt', 'r') as f:
    document = f.read()
# create mapping of image to captions
mapping = {}
# process lines
for line in tqdm(document.split('\n')):
    # split the line by comma(,)
    tokens = line.split(',')
    if len(line) < 2:
        continue
    image_id, caption = tokens[0], tokens[1:]
    # remove extension from image ID
    image_id = image_id.split('.')[0]
    # convert caption list to string
    caption = " ".join(caption)
    # create list if needed
    if image_id not in mapping:
        mapping[image_id] = []
    # store the caption
    mapping[image_id].append(caption)



def clean(mapping):
    for key, captions in mapping.items():
        for i in range(len(captions)):
            # take one caption at a time
            caption = captions[i]
            # preprocessing steps
            # convert to lowercase
            caption = caption.lower()
            # delete digits, special chars, etc.,
            caption = caption.replace('[^A-Za-z]', '')
            # delete additional spaces
            caption = caption.replace('\s+', ' ')
            # add start and end tags to the caption
            caption = 'startseq ' + " ".join([word for word in caption.split() if len(word)>1]) + ' endseq'
            captions[i] = caption


# preprocess the text
clean(mapping)
all_captions = []
for key in mapping:
    for caption in mapping[key]:
        all_captions.append(caption)


# create data generator to get data in batch (avoids session crash)
def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):
    # loop over images
    X1, X2, y = list(), list(), list()
    n = 0
    while 1:
        for key in data_keys:
            n += 1
            captions = mapping[key]
            # process each caption
            for caption in captions:
                # encode the sequence
                seq = tokenizer.texts_to_sequences([caption])[0]
                # split the sequence into X, y pairs
                for i in range(1, len(seq)):
                    # split into input and output pairs
                    in_seq, out_seq = seq[:i], seq[i]
                    # pad input sequence
                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                    # encode output sequence
                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]

                    # store the sequences
                    X1.append(features[0])
                    X2.append(in_seq)
                    y.append(out_seq)
            if n == batch_size:
                X1, X2, y = np.array(X1), np.array(X2), np.array(y)
                yield {"image": X1, "text": X2}, y
                X1, X2, y = list(), list(), list()
                n = 0
print('hi')


def cnn_bilstm(vocab_size, max_length):

    # CNN for image feature extraction
    image_input = Input(shape=(250, 250, 3), name="image")
    cnn_layer1 = Conv2D(32, (3, 3), activation='relu')(image_input)
    # pooling_layer1 = MaxPooling2D(pool_size=(2, 2))(cnn_layer1)
    # cnn_layer2 = Conv2D(64, (3, 3), activation='relu')(pooling_layer1)
    # pooling_layer2 = MaxPooling2D(pool_size=(2, 2))(cnn_layer2)
    # cnn_layer3 = Conv2D(128, (3, 3), activation='relu')(pooling_layer2)
    pooling_layer3 = MaxPooling2D(pool_size=(2, 2))(cnn_layer1)
    flatten_layer = Flatten()(pooling_layer3)
    dropout_layer1 = Dropout(0.4)(flatten_layer)
    dense_layer1 = Dense(64, activation='relu')(dropout_layer1)

    # Sequence feature layers
    text_input = Input(shape=(max_length,), name="text")
    embedding_layer = Embedding(vocab_size, 32, mask_zero=True)(text_input)
    dropout_layer2 = Dropout(0.4)(embedding_layer)
    bilstm_layer = Bidirectional(LSTM(32))(dropout_layer2)

    # Decoder model
    decoder1 = add([dense_layer1, bilstm_layer])
    decoder2 = Dense(256, activation='relu')(decoder1)
    output = Dense(vocab_size, activation='softmax')(decoder2)

    model = Model(inputs=[image_input, text_input], outputs=output)
    model.compile(loss='categorical_crossentropy', optimizer='adam')

    # # Plot the model
    # plot_model(model, show_shapes=True)
    return model


features = np.load('features.npy')
# tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1
print('vocab_size')
# get maximum length of the caption available
max_length = max(len(caption.split()) for caption in all_captions)
print(max_length)

image_ids = list(mapping.keys())
split = int(len(image_ids) * 0.90)
train = image_ids[:split]
test = image_ids[split:]

########## model
model = cnn_bilstm(vocab_size, max_length)
# train the model
epochs = 20
batch_size = 32
steps = len(train) // batch_size

for i in range(epochs):
    # create data generator
    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)
    # fit for one epoch
    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)
model.save('model.h5')



"""
import pandas as pd

with open('new.txt', 'r') as f:
    doc = f.read()


# Split the data into lines
lines = doc.strip().split('\n')

# Split each line into image ID and caption
records = [line.split(' ', 1) for line in lines]

# Create a DataFrame
df = pd.DataFrame(records, columns=['image_id', 'caption'])

# Display the DataFrame
print(df)


helpful reference link ---------- https://github.com/aswintechguy/Deep-Learning-Projects/blob/main/Image%20Caption%20Generator%20-%20Flickr%20Dataset/Image%20Caption%20Generator%20-%20Flickr%20Dataset%20-%20CNN-LSTM.ipynb
"""
